import { useState, useCallback, useRef, useEffect } from 'react';

export type RecognitionState = 'idle' | 'listening' | 'processing' | 'error';

interface UseSpeechRecognitionReturn {
    state: RecognitionState;
    transcript: string;
    error: string | null;
    isSupported: boolean;
    startListening: () => void;
    stopListening: () => void;
}

export function useSpeechRecognition(): UseSpeechRecognitionReturn {
    const [state, setState] = useState<RecognitionState>('idle');
    const [transcript, setTranscript] = useState('');
    const [error, setError] = useState<string | null>(null);

    const recognitionRef = useRef<SpeechRecognition | null>(null);

    // Check browser support
    const isSupported = typeof window !== 'undefined' &&
        ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window);

    useEffect(() => {
        if (!isSupported) return;

        const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
        const recognition = new SpeechRecognition();

        // Configure for child-friendly single-phrase capture
        recognition.continuous = false;
        recognition.interimResults = true;
        recognition.maxAlternatives = 1;

        // Auto-detect language (Russian or English)
        recognition.lang = navigator.language.startsWith('ru') ? 'ru-RU' : 'en-US';

        recognition.onstart = () => {
            setState('listening');
            setError(null);
            console.log('ðŸŽ¤ Speech recognition started');
        };

        recognition.onresult = (event: SpeechRecognitionEvent) => {
            const results = event.results;
            const lastResult = results[results.length - 1];
            const transcriptText = lastResult[0].transcript;

            setTranscript(transcriptText);

            if (lastResult.isFinal) {
                setState('processing');
                console.log('ðŸ“ Final transcript:', transcriptText);
            }
        };

        recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
            console.error('Speech recognition error:', event.error);
            setState('error');

            switch (event.error) {
                case 'no-speech':
                    setError('Ð¯ Ð½Ðµ ÑƒÑÐ»Ñ‹ÑˆÐ°Ð». ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·!');
                    break;
                case 'audio-capture':
                    setError('ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ!');
                    break;
                case 'not-allowed':
                    setError('Ð Ð°Ð·Ñ€ÐµÑˆÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½!');
                    break;
                default:
                    setError('Ð§Ñ‚Ð¾-Ñ‚Ð¾ Ð¿Ð¾ÑˆÐ»Ð¾ Ð½Ðµ Ñ‚Ð°Ðº. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·!');
            }
        };

        recognition.onend = () => {
            if (state === 'listening') {
                setState('idle');
            }
            console.log('ðŸŽ¤ Speech recognition ended');
        };

        recognitionRef.current = recognition;

        return () => {
            recognition.abort();
        };
    }, [isSupported]);

    const startListening = useCallback(() => {
        if (!recognitionRef.current) return;

        setTranscript('');
        setError(null);

        try {
            recognitionRef.current.start();
        } catch (e) {
            console.error('Failed to start recognition:', e);
        }
    }, []);

    const stopListening = useCallback(() => {
        if (!recognitionRef.current) return;
        recognitionRef.current.stop();
    }, []);

    return {
        state,
        transcript,
        error,
        isSupported,
        startListening,
        stopListening
    };
}
